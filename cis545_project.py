# -*- coding: utf-8 -*-
"""CIS545_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E7zjFQzqQL_l_rtL9GWxSLfUYTT1La-k

#**Introduction**

---



Hi! Welcome to Edoardo, Benjamin, and Joshua's CIS 545 final project.

In the advent of the era of fake news and misinformation, which bad actors largely take to socia media platforms to spread, we aim to uncover insights that can help social media platforms more quickly identify potentially problematic and disruptive posts.

Specifically, our project will utilize data on tweets discussing climate change -- a hotly debated and politicized topic, about which skepticism is often expressed and lies often amplified -- downloaded from Kaggle. We look at tweets to explore sentiment, engagement patterns, and build predictive models to capture the most pertinent attributes that can help us catch bad actors before social media posts cause irreparable damage -- whether financially or reputationally to the social media platform, or more broadly in shaping unfavorable, unscientific views in society.

Social media plays a crucial role in shaping public opinion, and studying climate change discussions on one of the most influential platforms can provide insights for not only the likes of Twitter or other social media companies, but also policymakers, advocacy groups, and researchers.

#Data Preprocessing, Feature Engineering & Exploratory Data Analysis

##Data That Will Be Used

For this project we will be using the following Kaggle datasets:


*  [The Climate Change Twitter](https://www.kaggle.com/datasets/deffro/the-climate-change-twitter-dataset/?select=The+Climate+Change+Twitter+Dataset.csv)
*  [Global Disasters 2007-2020](https://www.kaggle.com/datasets/deffro/the-climate-change-twitter-dataset/?select=disasters.csv)
*  [Social Media Sentiment and Climate Change](https://www.kaggle.com/datasets/thedevastator/social-media-sentiment-and-climate-change?select=Dataset.csv)

### Downloading The Data

To start, Colab will connect to Kaggle in order to download the datasets for this project.
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from geopy.geocoders import Nominatim

# Mount your google drive
from google.colab import drive
drive.mount('/content/drive')

# Connect to kaggle
!mkdir ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

# Download datasets
!!kaggle datasets download -d deffro/the-climate-change-twitter-dataset
!!kaggle datasets download -d thedevastator/social-media-sentiment-and-climate-change

!unzip /content/the-climate-change-twitter-dataset.zip
!unzip /content/social-media-sentiment-and-climate-change.zip

"""## Climate Change Tweets Data - Preprocessing

The first dataset we will read and process is our climate_tweets dataframe. This data consists of tweets about climate change. The text of the tweet is not provided in this dataset, but each tweet does contain:

*   when it was created
*   location of where it was created
*   topic discussed in the tweet
*   sentiment score
*   climate change stance
*   gender of Twitter user
*   whether it is aggressive or not
*   temperature deviation relative to temperature average of the 1950s - 1980s at the time and location of the created tweet

We will then later join this data with disasters data on time (year, month). We will then later use this information to build models to predict whether a tweet contains aggressive language.

Let's first read 'The Climate Change Twitter Dataset.csv' and save it to climate_tweets_df.

We will also take a look at the dataframe itself, its data types, and its number of rows.
"""

# Read csv file and save it to dataframe
climate_tweets_df = pd.read_csv('The Climate Change Twitter Dataset.csv')

# Take a look at the first several rows of climate_tweets_df
climate_tweets_df.head()

# Take a look at the datatypes of climate_tweets_df
climate_tweets_df.dtypes

# Take a look at the number of rows in climate_tweets_df
climate_tweets_df.shape[0]

"""### Data Cleaning: Handling Nulls and Filtering

Let's remove a lot of the noise by dropping nulls and duplicates in the data.

We will also cast some of its columns to string data types.

This is necessary for our machine learning later on.
"""

# Drop nulls and duplicates
climate_tweets_df.dropna(inplace=True)
climate_tweets_df.drop_duplicates(inplace=True)

# Drop id column now that we know every tweet is unique
climate_tweets_df.drop(columns=['id'], inplace=True)

# Cast columns to type 'string' for columns: 'topic', 'stance', 'gender', 'aggressiveness'
climate_tweets_df = climate_tweets_df.astype({'topic': 'string', 'stance': 'string', 'gender': 'string', 'aggressiveness': 'string'})

"""####Visualization: Global Distribution of Tweets

Using geopandas, let's visualize the distribution of our tweets on a world map.
"""

import geopandas
from geopy.geocoders import Nominatim
gdf = geopandas.GeoDataFrame(climate_tweets_df, geometry=geopandas.points_from_xy(climate_tweets_df['lng'], climate_tweets_df['lat']))
world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
ax = world.plot(figsize=(18,20), legend = True, color='white', edgecolor='black',
                legend_kwds={'label': "Crawled Tweets Distribution",
                          'orientation': "horizontal"})
gdf.plot(ax=ax, color='red')
plt.title("Crawled Tweets Distribution")
plt.show()

"""For the purpose of our study/analysis, we will focus on tweets within the United States (includes all 50 states), which comprise the majority of the tweets in our current dataframe. We will save the tweets within the United States in a new dataframe: filtered_climate_tweets_df."""

# Keep latitude and longitude within contiguous United States (Alaska and Hawaii included)
filtered_climate_tweets_df = climate_tweets_df[
    (
        (climate_tweets_df['lat'] >= 24.396308) & (climate_tweets_df['lat'] <= 49.384358) &
        (climate_tweets_df['lng'] >= -124.848974) & (climate_tweets_df['lng'] <= -66.885444)
    )
    |
    (
        (climate_tweets_df['lat'] >= 18.647399) & (climate_tweets_df['lat'] <= 22.662554) &
        (climate_tweets_df['lng'] >= -161.057672) & (climate_tweets_df['lng'] <= -154.308715)
    )
    |
    (
        (climate_tweets_df['lat'] >= 53.356518) & (climate_tweets_df['lat'] <= 71.599699) &
        (climate_tweets_df['lng'] >= -168.392138) & (climate_tweets_df['lng'] <= -141.004329)
    )
]

"""####Visualization: Distribution of Tweets in USA

We will again visualize the distribution of our tweets on a world map to show that only tweets in the United States remain in our new dataframe.
"""

import geopandas
from geopy.geocoders import Nominatim

gdf = geopandas.GeoDataFrame(filtered_climate_tweets_df, geometry=geopandas.points_from_xy(filtered_climate_tweets_df['lng'], filtered_climate_tweets_df['lat']))
world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
ax = world.plot(figsize=(18,20), legend = True, color='white', edgecolor='black',
                legend_kwds={'label': "Crawled Tweets Distribution (US)",
                          'orientation': "horizontal"})
gdf.plot(ax=ax, color='red')
plt.title("Crawled Tweets Distribution (US)")
plt.show()

!pip install descartes

"""####Visualization: Number of Tweets per Gender

Let's check the number of tweets created by each gender.
"""

gender_counts = filtered_climate_tweets_df.groupby('gender').size().reset_index(name='count')

# Using seaborn
sns.set(style="whitegrid")
plt.figure(figsize=(6, 4))

# Plotting the bar chart
ax = sns.barplot(x='gender', y='count', data=gender_counts, palette="viridis")
ax.set(title='Number of Tweets by Gender', xlabel='Gender', ylabel='Number of Tweets')

# Adding data labels on top of the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8)

plt.show()

"""From the figure above, since there are significantly less tweets created by Twitter accounts with no specified gender, we will remove those tweets."""

# Remove tweets created by accounts with an undefined gender
filtered_climate_tweets_df = filtered_climate_tweets_df[filtered_climate_tweets_df['gender'] != 'undefined']

"""### Categorizing the USA into 3 Regions

Because it is geographically different throughout the United States, we will split the United States into 3 regions.

*   West: This region consists of all locations to the left of and including the Rocky Mountains. Alaska and Hawaii will be in this region.
*   East: This region consists of all locations to the right of and including the Appalachian Mountains.
*   Central: This region consists of all locations between the Rocky Mountains and the Appalachian Mountains.

We have provided the 2 maps used to determine the longitudes of where the United States would be split into 3 regions.

*  [Topography of the United States](https://cdn.britannica.com/03/111403-004-5BCA19DF.jpg)
*  [Longitudes of the United States](https://athometutoringservices.com/wp-content/uploads/2023/09/latitude_and_Longitude_Map.gif)
"""

# Define function to categorize a United States location into one of 3 regions: west, central, east
def determine_US_region(longitude):
  if longitude <= -105:
    return 'west'
  elif longitude >= -85:
    return 'east'
  else:
    return 'central'

# Categorize each United States location into one of 3 regions: 'west', 'central', 'east'
filtered_climate_tweets_df['region'] = filtered_climate_tweets_df['lng'].apply(determine_US_region)

# Cast column 'region' to type 'string'
filtered_climate_tweets_df = filtered_climate_tweets_df.astype({'region': 'string'})

# Remove latitude and longitude coordinates since location of tweets have been narrowed down to United States and each location is put into one of 3 regions
filtered_climate_tweets_df = filtered_climate_tweets_df.drop(columns=['lat', 'lng'])

"""#### Visualization: Number of Tweets per Region

Let's see the number of tweets per region.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Number of tweets from central, east, and west regions of the US
region_counts = filtered_climate_tweets_df.groupby('region').size().reset_index(name='count')

sns.set(style="whitegrid")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))  # Creating subplots

bar_order = ['west', 'central', 'east']

# Create a dictionary to map regions to colors
region_colors = dict(zip(bar_order, sns.color_palette("viridis", len(bar_order))))

bar_plot = sns.barplot(x='region', y='count', data=region_counts, order=bar_order, palette=region_colors, width=0.6, ax=ax1)
ax1.set(title='Number of Tweets by Region', xlabel='Region', ylabel='Number of Tweets')

# Extracting the color palette from the bar chart
bar_colors = [region_colors[region] for region in region_counts['region']]

for p, color in zip(bar_plot.patches, bar_colors):
    ax1.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                 ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8, color='black')

# Plotting the pie chart with matching colors
wedgeprops = dict(width=0.3, edgecolor='w')
patches, texts, autotexts = ax2.pie(region_counts['count'], labels=region_counts['region'], autopct='%1.1f%%',
                                     startangle=90, colors=bar_colors, wedgeprops=wedgeprops, textprops=dict(color='black'))

# Cleanup of the labels
for autotext, label in zip(autotexts, region_counts['region']):
    autotext.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round'))
    #autotext.set_text(f'{label}\n{autotext.get_text()}')  # Add region label below the percentage

ax2.set(title='Distribution of Tweets by Region')
ax2.set_ylabel('')
ax2.tick_params(left=False)

plt.tight_layout()
plt.show()

"""From the figure above, although we divided the United States into 3 regions with approximately the same area, it is expected that the east region contains the most tweets because majority of the USA population lives in the eastern USA states. There is a bit more tweets in the west region than the central region.

###Data Cleaning: Extracting Year/Month of Tweets

We will be extracting the month and year of the tweets. Since we are working with an abundance of tweets spanning over several years, we will not be using the day of the tweet.
"""

# Cast column to type 'datetime': 'created_at'
filtered_climate_tweets_df['created_at'] = pd.to_datetime(filtered_climate_tweets_df['created_at'])

# Extract year, month created for each tweet
filtered_climate_tweets_df['year_created'] = filtered_climate_tweets_df['created_at'].apply(lambda x: x.year)
filtered_climate_tweets_df['month_created'] = filtered_climate_tweets_df['created_at'].apply(lambda x: x.month)

"""Let's take a look at the years for when the climate change tweets were made."""

filtered_climate_tweets_df['year_created'].unique()

"""We will be using tweets from 2017 and onwards because these tweets are most representative of the current views on climate change."""

# Keep only tweets from 2017 and onwards
filtered_climate_tweets_df = filtered_climate_tweets_df[filtered_climate_tweets_df['year_created'] >= 2017]

"""####Visualization: Temporal Trends in Number of Tweets

Let's see how the number of tweets change as time moves forward.
"""

# Extract date and count tweets per day
daily_tweet_counts = filtered_climate_tweets_df.resample('D', on='created_at').size()

# Plotting the time series graph
plt.figure(figsize=(12, 6))
daily_tweet_counts.plot(linewidth=2)
plt.title('Temporal Trends in Number of Tweets')
plt.xlabel('Date')
plt.ylabel('Number of Tweets')
plt.show()

"""From the figure above, we can see that the number of tweets is consistently lower than 10,000 for nearly all of 2017. Near the end of 2017 and moving into 2018, the number of tweets go up significantly. In December 2017, the number of tweets is at its maximum in the plot at nearly 50,000. Throughout majority of 2018, the number of tweets is consistently lower than 10,000. In the second half of 2018, the number of tweets goes above 10,000 several times. However, in 2019, almost all months have nearly 0 tweets.

We are now done with cleaning and filtering climate_tweets_df.

Let's take a look at the current state of filtered_climate_tweets_df before we one-hot encode this dataframe.
"""

filtered_climate_tweets_df.head()

# Take a look at the final number of tweets we will be working with in this dataframe
filtered_climate_tweets_df.shape[0]

"""### One-Hot Encoding

We will one-hot encode filtered_climate_tweets_df and save it as encoded_climate_tweets_df.

Before we do that, let's find out which are the binary categorical variables. So that after one-hot encoding, we can remove 1 of its columns for those binary categorical variables to prevent redundancy and to avoid later trouble with our models.
"""

filtered_climate_tweets_df.nunique()

"""The categorical variables to one-hot encode include:

*   topic
*   stance
*   gender
*   aggressiveness
*   region

The binary categorical variables are:

*   gender
*   aggressiveness
"""

# One-hot encode categorical variables: 'topic', 'stance', 'gender', 'aggressiveness', 'region'
encoded_climate_tweets_df = pd.get_dummies(data=filtered_climate_tweets_df, columns=['topic', 'stance', 'gender', 'aggressiveness', 'region'])

# Take a look at the first couple rows of the one-hot encoded dataframe
encoded_climate_tweets_df.head(3)

# Take a look at the new columns in encoded_climate_tweets_df
encoded_climate_tweets_df.dtypes

# Drop 1 column for each of the binary categorical variables: gender, aggressiveness
# Drop 'created_at' column
encoded_climate_tweets_df = encoded_climate_tweets_df.drop(columns=['created_at', 'gender_male', 'aggressiveness_not aggressive'])

# Rename column names with spaces and replace them with names that have underscore
encoded_climate_tweets_df = encoded_climate_tweets_df.rename(columns={'topic_Donald Trump versus Science': 'topic_Donald_Trump_versus_Science',
                                                                      'topic_Global stance': 'topic_Global_stance',
                                                                      'topic_Ideological Positions on Global Warming': 'topic_Ideological_Positions_on_Global_Warming',
                                                                      'topic_Impact of Resource Overconsumption': 'topic_Impact_of_Resource_Overconsumption',
                                                                      'topic_Importance of Human Intervantion': 'topic_Importance_of_Human_Intervantion',
                                                                      'topic_Seriousness of Gas Emissions': 'topic_Seriousness_of_Gas_Emissions',
                                                                      'topic_Significance of Pollution Awareness Events': 'topic_Significance_of_Pollution_Awareness_Events',
                                                                      'topic_Undefined / One Word Hashtags': 'topic_Undefined_or_One_Word_Hashtags',
                                                                      'topic_Weather Extremes': 'topic_Weather_Extremes'})

"""## Disasters Data - Preprocessing

The next dataset we will read and process is our disasters dataframe. This data consists of disasters from 2007 to 2020.

We will later join this data with the previously preprocessed climate_tweets dataframe on time (year, month). We will then use this information to build models to predict whether a tweet contains aggressive language.

Let's first read 'disasters.csv' and save it to disasters_df.

We will also take a look at the dataframe itself, its data types, and its number of rows.
"""

# Read in disasters csv
disasters_df = pd.read_csv('disasters.csv')

# Take a look at the first several rows of disasters_df
disasters_df.head()

# Take a look at the datatypes of disasters_df
disasters_df.dtypes

# Take a look at the number of rows in disasters_df
disasters_df.shape[0]

"""###Data Cleaning: Handling Nulls and Filtering

There are 2 columns that we are interested in: 'Disaster Type' and 'Disaster Subgroup'.

Since we are analyzing tweets related to climate change, we want to choose 1 of the 2 columns listed above that will be better for filtering out disasters not affected by climate change.

Let's take a look at the unique values of 'Disaster Type' and 'Disaster Subgroup'.
"""

disasters_df['Disaster Type'].unique()

disasters_df['Disaster Subgroup'].unique()

"""By the definitions of the unique values of 'Disaster Subgroup', it would be best to use this column to filter out disasters not affected by climate change. Specifically, we can remove geophysical disasters.

First, we will keep the columns: 'Country', 'start_date', 'Disaster Subgroup'.

Even though we will not be filtering for disasters in specific countries, having a disaster listed with a country can indicate that the disaster was large enough to be covered on social media at some point during the disaster.

We also decided to choose 'start_date' instead of 'end_date' because the dates around the start date usually affect the most people in that location; hence, people on social media are more likely to discuss about it on social media.

After filtering disasters not affected by climate change, we will save it in a new dataframe: filtered_disasters_df.
"""

disasters_df = disasters_df[['Country', 'start_date', 'Disaster Subgroup']]
disasters_df = disasters_df.rename(columns={'Disaster Subgroup': 'disaster_subgroup'})

# Cast column to type 'string': 'start_date', 'disaster_subgroup'
disasters_df = disasters_df.astype({'start_date': 'string', 'disaster_subgroup': 'string'})

# Remove nulls and duplicates
disasters_df = disasters_df.dropna()
disasters_df = disasters_df.drop_duplicates()

# We did not look for disasters in specific countries, but used this column as a way
# to keep disasters with a magnitude large enough to have individuals discuss it on social media
disasters_df = disasters_df.drop(columns=['Country'])

# Remove disasters not affected by climate change
filtered_disasters_df = disasters_df[disasters_df['disaster_subgroup'] != 'Geophysical']

# Take a peek of the current state of filtered_disasters_df
filtered_disasters_df.head(5)

# Take a look at the size of the current filtered_disasters_df
filtered_disasters_df.shape[0]

"""###Data Cleaning: Extracting Year/Month of Disasters

Let's extract the year and month of each disaster in filtered_disasters_df.

We will later group filtered_disasters_df by these 2 new columns to get the number of disasters in each subgroup for each year and month.
"""

# Cast column to type 'datetime': 'start_date'
filtered_disasters_df['date'] = pd.to_datetime(filtered_disasters_df['start_date'])
filtered_disasters_df = filtered_disasters_df.drop(columns=['start_date'])

# Extract year, month of each disaster
filtered_disasters_df['year'] = filtered_disasters_df['date'].apply(lambda x: x.year)
filtered_disasters_df['month'] = filtered_disasters_df['date'].apply(lambda x: x.month)
filtered_disasters_df = filtered_disasters_df.drop(columns=['date'])

"""Let's take a look at the years for the reported disasters."""

filtered_disasters_df['year'].unique()

"""Since we are working with climate change tweets from 2017 and onwards, we will be working with disasters also from 2017 and onwards."""

# Keep only disasters from 2017 and onwards
filtered_disasters_df = filtered_disasters_df[filtered_disasters_df['year'] >= 2017]

"""Let's take a look at the current state of filtered_disasters_df before we one-hot encode this dataframe."""

# Take a peek at the current state of filtered_disasters_df
filtered_disasters_df.head()

"""###One-Hot Encoding

We will one-hot encode filtered_climate_disasters_df and save it as encoded_climate_disasters_df.

The categorical variable to one-hot encode is disaster_subgroup.
"""

# One-hot encode disaster subgroup
encoded_disasters_df = pd.get_dummies(data=filtered_disasters_df, columns=['disaster_subgroup'])

# Take a look at one-hot encoded disaster subgroups with the year and month of occurrence
encoded_disasters_df.head()

"""###Getting Number of Disasters for each Year/Month

We will group encoded_disasters_df by 'year' and 'month' to get the number of disasters in each subgroup for each year and month.
"""

# Group by 'year' and 'month' and get sum for number of disasters in each subgroup
encoded_disasters_df = encoded_disasters_df.groupby(by=['year', 'month']).agg({'disaster_subgroup_Climatological': 'sum',
                                                                               'disaster_subgroup_Hydrological': 'sum',
                                                                               'disaster_subgroup_Meteorological': 'sum'}).reset_index()

# Rename columns to appropriately convey what the column stores
encoded_disasters_df = encoded_disasters_df.rename(columns={'disaster_subgroup_Climatological': 'num_climatological_disasters',
                                                            'disaster_subgroup_Hydrological': 'num_hydrological_disasters',
                                                            'disaster_subgroup_Meteorological': 'num_meteorological_disasters'})

"""Let's take a look at the first several rows of the preprocessed disasters_df."""

encoded_disasters_df.head()

"""### Visualizing the Disasters Data

Let's now visualize the disasters data to get a better idea of disasters count/trends over time.

To start, let's see the total number of disasters over time.
"""

# Calculate total disasters by summing the three categories
encoded_disasters_df['total_disasters'] = (
    encoded_disasters_df['num_climatological_disasters'] +
    encoded_disasters_df['num_hydrological_disasters'] +
    encoded_disasters_df['num_meteorological_disasters']
)

# Plotting
plt.figure(figsize=(12, 8))

# Plot total disasters
plt.plot(encoded_disasters_df['year'].astype(str) + '-' + encoded_disasters_df['month'].astype(str),
         encoded_disasters_df['total_disasters'],
         label='Total Disasters', marker='o', color='purple')

# Customize the plot
plt.title('Total Number of Disasters Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Number of Disasters')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.grid(True)

n = len(encoded_disasters_df)
plt.xticks(range(0, n, max(n // 20, 1)))

# Show the plot
plt.tight_layout()
plt.show()

"""From the figure above, in 2017 and 2018, there were large fluctuations in the total number of disasters. In 2019 and onwards, the total number of disasters did not have many fluctuations and were higher on average compared to 2017/2018.

Let's now see the number of disasters by subgroup over time.
"""

# Plotting
plt.figure(figsize=(12, 8))

# Plot climatological disasters
plt.plot(encoded_disasters_df['year'].astype(str) + '-' + encoded_disasters_df['month'].astype(str),
         encoded_disasters_df['num_climatological_disasters'],
         label='Climatological Disasters', marker='o')

# Plot hydrological disasters
plt.plot(encoded_disasters_df['year'].astype(str) + '-' + encoded_disasters_df['month'].astype(str),
         encoded_disasters_df['num_hydrological_disasters'],
         label='Hydrological Disasters', marker='o')

# Plot meteorological disasters
plt.plot(encoded_disasters_df['year'].astype(str) + '-' + encoded_disasters_df['month'].astype(str),
         encoded_disasters_df['num_meteorological_disasters'],
         label='Meteorological Disasters', marker='o')

# Customize the plot
plt.title('Number of Disasters by Subgroup Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Number of Disasters')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.legend()
plt.grid(True)

n = len(encoded_disasters_df)
plt.xticks(range(0, n, max(n // 20, 1)))

# Show the plot
plt.tight_layout()
plt.show()

"""From the figure above, there are 3 disaster subgroups with each having a different trend. For the climatological disasters, the number of disasters were consistently below 5 with it being 0 at many of the month/year. For the hydrological disasters, there was a general increasing trend. For the meteorological disasters, there were many fluctuations but has a general trend that is not increasing or decreasing.

Let's use also a heatmap to help us better visualize the amount of climatological disasters over time.
"""

# Pivot the dataframe for a heatmap
heatmap_data = encoded_disasters_df.pivot('month', 'year', 'num_climatological_disasters')

# Plotting heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, cmap='Blues', annot=True, fmt='g', cbar_kws={'label': 'Number of Disasters'})
plt.title('Climatological Disasters Over Time')
plt.xlabel('Year')
plt.ylabel('Month')

# Show the plot
plt.tight_layout()
plt.show()

"""Let's use also a heatmap to help us better visualize the amount of hydrological disasters over time."""

# Pivot the dataframe for a heatmap
heatmap_data = encoded_disasters_df.pivot('month', 'year', 'num_hydrological_disasters')

# Plotting heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, cmap='Oranges', annot=True, fmt='g', cbar_kws={'label': 'Number of Disasters'})
plt.title('Hydrological Disasters Over Time')
plt.xlabel('Year')
plt.ylabel('Month')

# Show the plot
plt.tight_layout()
plt.show()

"""Let's use also a heatmap to help us better visualize the amount of meteorological disasters over time."""

# Pivot the dataframe for a heatmap
heatmap_data = encoded_disasters_df.pivot('month', 'year', 'num_meteorological_disasters')

# Plotting heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, cmap='Greens', annot=True, fmt='g', cbar_kws={'label': 'Number of Disasters'})
plt.title('Meteorological Disasters Over Time')
plt.xlabel('Year')
plt.ylabel('Month')

# Show the plot
plt.tight_layout()
plt.show()

"""## Joining Climate Change Tweets and Disasters Data

###Setting up Apache Spark

Since encoded_climate_tweets_df has approximately 2.8 million rows and 22 columns, Apache Spark would be the safer option to prevent an out-of-memory error when joining encoded_climate_tweets_df with encoded_disasters_df.

We will be running Apache Spark on our local machine.
"""

# Commented out IPython magic to ensure Python compatibility.
# 
# %%capture
# 
# !apt install libkrb5-dev
# !wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar xf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install findspark
# !pip install sparkmagic
# !pip install pyspark
# ! pip install pyspark --user
# ! pip install seaborn --user
# ! pip install plotly --user
# ! pip install imageio --user
# ! pip install folium --user
#

import pyspark
from pyspark.sql import SQLContext
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

spark = SparkSession.builder.appName('cis545_project').getOrCreate()
sqlContext = SQLContext(spark)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

# SQLite RDBMS
import sqlite3

import os
os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""### Converting Climate Change Tweets Data to a Spark Dataframe

Let's convert encoded_climate_tweets_df to a Spark dataframe.

We will use dtypes of encoded_climate_tweets_df as aid to provide a schema for the Spark dataframe.
"""

encoded_climate_tweets_df.dtypes

"""When creating the schema, make sure that column: 'aggressiveness_aggressive' is a DoubleType().

This will be later important for our models.
"""

# Convert 'aggressiveness_aggressive' to a double
encoded_climate_tweets_df['aggressiveness_aggressive'] =  encoded_climate_tweets_df['aggressiveness_aggressive'] * 1.0

climate_tweets_schema = StructType([
	  StructField("sentiment", DoubleType(), nullable=True),
    StructField("temperature_avg", DoubleType(), nullable=True),
    StructField("year_created", IntegerType(), nullable=True),
	  StructField("month_created", IntegerType(), nullable=True),
    StructField("topic_Donald_Trump_versus_Science", IntegerType(), nullable=True),
    StructField("topic_Global_stance", IntegerType(), nullable=True),
	  StructField("topic_Ideological_Positions_on_Global_Warming", IntegerType(), nullable=True),
	  StructField("topic_Impact_of_Resource_Overconsumption", IntegerType(), nullable=True),
    StructField("topic_Importance_of_Human_Intervantion", IntegerType(), nullable=True),
    StructField("topic_Politics", IntegerType(), nullable=True),
	  StructField("topic_Seriousness_of_Gas_Emissions", IntegerType(), nullable=True),
    StructField("topic_Significance_of_Pollution_Awareness_Events", IntegerType(), nullable=True),
    StructField("topic_Undefined_or_One_Word_Hashtags", IntegerType(), nullable=True),
    StructField("topic_Weather_Extremes", IntegerType(), nullable=True),
	  StructField("stance_believer", IntegerType(), nullable=True),
	  StructField("stance_denier", IntegerType(), nullable=True),
    StructField("stance_neutral", IntegerType(), nullable=True),
    StructField("gender_female", IntegerType(), nullable=True),
    StructField("aggressiveness_aggressive", DoubleType(), nullable=True),
    StructField("region_central", IntegerType(), nullable=True),
    StructField("region_east", IntegerType(), nullable=True),
    StructField("region_west", IntegerType(), nullable=True)
])

climate_tweets_sdf = spark.createDataFrame(encoded_climate_tweets_df, climate_tweets_schema)

climate_tweets_sdf.show(5)

"""###Converting Disasters Data to a Spark Dataframe

Let's convert encoded_disasters_df to a Spark dataframe.

We will use dtypes of encoded_disasters_df as aid to provide a schema for the Spark dataframe.
"""

encoded_disasters_df.dtypes

disasters_schema = StructType([
    StructField("year", IntegerType(), nullable=True),
    StructField("month", IntegerType(), nullable=True),
    StructField("num_climatological_disasters", IntegerType(), nullable=True),
    StructField("num_hydrological_disasters", IntegerType(), nullable=True),
    StructField("num_meteorological_disasters", IntegerType(), nullable=True)
])

disasters_sdf = spark.createDataFrame(encoded_disasters_df, disasters_schema)

disasters_sdf.show(5)

"""### Joining Climate Change Tweets and Disasters Data on Apache Spark

We will have climate_tweets_sdf left join with disasters_sdf on 'year' and 'month'.

As a result, for each climate change tweet, we can now also observe the number of disasters in each subgroup when that tweet was made.

This will be later important for our models.
"""

# Join climate_tweets_sdf and disasters_sdf on 'year' and 'month'
climate_tweets_sdf.createOrReplaceTempView('climate_tweets')
disasters_sdf.createOrReplaceTempView('disasters')

query = '''
        SELECT
          ct.sentiment AS sentiment,
          ct.temperature_avg AS temperature_avg,
          ct.year_created AS year_created,
          ct.month_created AS month_created,
          d.num_climatological_disasters AS num_climatological_disasters,
          d.num_hydrological_disasters AS num_hydrological_disasters,
          d.num_meteorological_disasters AS num_meteorological_disasters,
          ct.topic_Donald_Trump_versus_Science AS topic_Donald_Trump_versus_Science,
          ct.topic_Global_stance AS topic_Global_stance,
          ct.topic_Ideological_Positions_on_Global_Warming AS topic_Ideological_Positions_on_Global_Warming,
          ct.topic_Impact_of_Resource_Overconsumption AS topic_Impact_of_Resource_Overconsumption,
          ct.topic_Importance_of_Human_Intervantion AS topic_Importance_of_Human_Intervantion,
          ct.topic_Politics AS topic_Politics,
          ct.topic_Seriousness_of_Gas_Emissions AS topic_Seriousness_of_Gas_Emissions,
          ct.topic_Significance_of_Pollution_Awareness_Events AS topic_Significance_of_Pollution_Awareness_Events,
          ct.topic_Undefined_or_One_Word_Hashtags AS topic_Undefined_or_One_Word_Hashtags,
          ct.topic_Weather_Extremes AS topic_Weather_Extremes,
          ct.stance_believer AS stance_believer,
          ct.stance_denier AS stance_denier,
          ct.stance_neutral AS stance_neutral,
          ct.gender_female AS gender_female,
          ct.aggressiveness_aggressive AS aggressiveness_aggressive,
          ct.region_central AS region_central,
          ct.region_east AS region_east,
          ct.region_west AS region_west
        FROM climate_tweets AS ct
          LEFT JOIN disasters AS d
            ON ct.year_created = d.year
            AND ct.month_created = d.month
        '''

climate_tweets_and_disasters_sdf = spark.sql(query)
climate_tweets_and_disasters_sdf.show(5)

"""Since there may be no data of disasters for a particular year and month when a climate change tweet was made, we will fill in the nulls with 0.

By filling in the nulls with 0 instead of removing those tweets from our dataframe, we can analyze different aspects of the tweets for when there are disasters vs when there are no reported disasters.
"""

climate_tweets_and_disasters_sdf = climate_tweets_and_disasters_sdf.fillna(0)

"""## Sentiment Tweets Data - Preprocessing

The final dataset we will read and process is our sentiment_tweets dataframe. This data consists of tweets about climate change, but unlike our climate_tweets data, actually includes the text of the tweets so we can do our own sentiment analysis, and information about retweets/favorites (i.e. activity).

We will then join this data with the disasters data on time (year, month) as well, so we know whether these tweets occurred while natural disasters were going on concurrently. We will then use this information, along with the computed sentiment scores, to build a linear regression model to predict the tweet's activity generated.

Let's first read 'Dataset.csv' and save it to sentiment_tweets_df.

We will also take a look at the dataframe itself, its data types, and its number of rows.
"""

# Read in social media sentiment dataset
sentiment_tweets_df = pd.read_csv('Dataset.csv')

# Take a look at the first several rows of sentiment_tweets_df
sentiment_tweets_df.head()

# Take a look at the datatypes of sentiment_tweets_df
sentiment_tweets_df.dtypes

# Take a look at the number of rows in sentiment_tweets_df
sentiment_tweets_df.shape[0]

"""### Data Cleaning: Handling Nulls and Filtering

We will:

*   keep the columns: 'created_at', 'text', 'favorite_count', 'retweet_count'
*   drop the nulls and duplicates
*   cast the columns into its appropriate data types
*   combine 'favorite_count' and 'retweet_count' into 1 column: 'activity_count'
"""

# Keep columns: 'created_at', 'text', 'favorite_count', 'retweet_count'
sentiment_tweets_df = sentiment_tweets_df[['created_at', 'text', 'favorite_count', 'retweet_count']]

# Remove nulls and duplicates
sentiment_tweets_df.dropna(inplace=True)
sentiment_tweets_df.drop_duplicates(inplace=True)

# Cast columns into type 'string': 'created_at', 'text'
sentiment_tweets_df = sentiment_tweets_df.astype({'created_at': 'string', 'text': 'string'})

# Cast column to type 'datetime': 'created_at'
sentiment_tweets_df['created_at'] = pd.to_datetime(sentiment_tweets_df['created_at'])

# Combine 'favorite_count' and 'retweet_count' into 1 column: 'activity_count'
sentiment_tweets_df['activity_count'] = sentiment_tweets_df['favorite_count'] + sentiment_tweets_df['retweet_count']
sentiment_tweets_df = sentiment_tweets_df.drop(columns=['favorite_count', 'retweet_count'])

"""Let's take a look at the current state of sentiment_tweets_df."""

sentiment_tweets_df.head()

"""###Data Cleaning: Extracting Original Tweets from Retweets

Since this dataset includes retweets as a separate tweet, we will be combining the original tweet with all of its retweets in order to get the total amount of activity for each unique tweet.

Note: Retweets are done in this format: 'RT @usernameOfOriginalTweet: original tweet goes here'

So, we will be extracting the original tweets from the retweets by looking for retweets in the above format.

We will:

*   determine if a tweet is a retweet or not
*   extract the original tweets from the retweets
*   group by the original tweets
    *   get the sum of activity count
    *   get the date for when the original tweet was made
"""

# Determine if a tweet is a retweet or not
sentiment_tweets_df['is_retweet'] = sentiment_tweets_df['text'].apply(lambda x: 1 if x.startswith('RT @') else 0)

# Separate sentiment_tweets_df into 2 dataframes: 1 for original tweets and 1 for retweets
sentiment_orig_tweets_df = sentiment_tweets_df[sentiment_tweets_df['is_retweet'] == 0]
sentiment_retweets_df = sentiment_tweets_df[sentiment_tweets_df['is_retweet'] == 1]

# Function used to extract original tweet from retweet
def get_original_tweet(text):
  idx = text.find(":")
  idx += 2
  return text[idx:]

# Extract the original tweets from all retweets
sentiment_retweets_df['text'] = sentiment_retweets_df['text'].apply(get_original_tweet)

# Drop column 'is_retweet' because we have extracted original tweet from retweet
sentiment_orig_tweets_df = sentiment_orig_tweets_df.drop(columns=['is_retweet'])
sentiment_retweets_df = sentiment_retweets_df.drop(columns=['is_retweet'])

# Concatenate the 2 dataframes to later group by 'text'
sentiment_tweets_df = pd.concat([sentiment_orig_tweets_df, sentiment_retweets_df])

# Group by 'text' and get sum of activity count and get date for when original tweet was made
sentiment_tweets_df = sentiment_tweets_df.groupby(by='text').agg({'created_at': 'min', 'activity_count': 'sum'}).reset_index()

"""Let's also take a look at the number of original tweets in this dataset."""

sentiment_tweets_df.shape[0]

"""###Data Cleaning: Extracting Year/Month of Tweets

Let's extract the year and month for each sentiment tweet.
"""

# Extract year, month created for each tweet
sentiment_tweets_df['year_created'] = sentiment_tweets_df['created_at'].apply(lambda x: x.year)
sentiment_tweets_df['month_created'] = sentiment_tweets_df['created_at'].apply(lambda x: x.month)
sentiment_tweets_df.drop(columns=['created_at'], inplace=True)

"""Let's take a look at the years for when the sentiment tweets were made."""

sentiment_tweets_df['year_created'].unique()

"""Let's also take a look at the current state of sentiment_tweets_df."""

sentiment_tweets_df.head()

"""###Sentiment Analysis on Tweets

####Cleaning Tweets

We will be using AFINN to perform our sentiment analysis on tweets.

Before performing sentiment analysis, we will:

*   clean the tweet by removing links and special characters
*   remove nulls
"""

# Import regex
import re

# Remove links and special characters using regex
# Inspired by https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/
sentiment_tweets_df['text'] = sentiment_tweets_df['text'].apply(lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", x))

# Determine if there is only whitespace after regex
# If so, replace empty text with None
sentiment_tweets_df['text'] = sentiment_tweets_df['text'].apply(lambda x: None if x.isspace() else x)

# Remove nulls
sentiment_tweets_df.dropna(inplace=True)

"""Let's take a look at the preprocessed sentiment_tweets_df before we perform sentiment analysis."""

sentiment_tweets_df.head()

"""####Sentiment Analysis using AFINN

Now that the tweets are ready for sentiment analysis, we will begin by installing/importing necessary tools to use AFINN.
"""

!pip install afinn
from afinn import Afinn

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

"""Since AFINN scores each word from [-5, 5], we will be scaling the scores to [-1, 1].

We will take advantage of Pandas Multiprocessing to parallelize computing the sentiment scores for the tweets.

"""

afinn = Afinn(language='en')

# Function utlizing AFINN to compute sentiment scores of tweets ranging [-1, 1]
def sentiment_score(text):
  words = word_tokenize(text)
  scores = [(afinn.score(w) / 5.0) for w in words]
  return round(sum(scores) / len(scores), 5)

pip install multiprocesspandas

from multiprocesspandas import applyparallel

# Compute sentiment scores of tweets with pandas multiprocessing
sentiment_tweets_df['score'] = sentiment_tweets_df['text'].apply_parallel(sentiment_score, num_processes=12)

"""####Visualization: Distribution of Sentiment Scores for Tweets

Let's take a look at the distribution of AFINN scores for the tweets.
"""

scores = sentiment_tweets_df[['score']]

sns.set_theme(style="darkgrid")
plt.figure(figsize=(8, 5))
ax = sns.histplot(scores, bins=30, kde=True)
ax.set_xlim([-1, 1])  # Adjust the limits based on your score range
plt.title('Distribution of Afinn Scores')
plt.xlabel('Afinn Score')
plt.ylabel('Frequency')
plt.show()

"""From the figure above, almost all of the Afinn scores are approximately 0. Hence, almost all of the tweets in this dataset have a neutral sentiment.

## Joining Sentiment Tweets and Disasters Data

We will be having sentiment_tweets_df left join with encoded_disasters_df (i.e. disasters_sdf) on the year and month.

Since we know that the years in sentiment_tweets_df are [2017, 2019] and the years in encoded_disasters_df are [2017, 2020], we can proceed with joining these 2 dataframes.

### Converting Sentiment Tweets to a Spark Dataframe

Let's convert sentiment_tweets_df to a Spark dataframe.

We will use dtypes of sentiment_tweets_df as aid to provide a schema for the Spark dataframe.
"""

sentiment_tweets_df.dtypes

sentiment_tweets_schema = StructType([
    StructField("text", StringType(), nullable=True),
    StructField("activity_count", IntegerType(), nullable=True),
    StructField("year_created", IntegerType(), nullable=True),
    StructField("month_created", IntegerType(), nullable=True),
    StructField("score", DoubleType(), nullable=True)
])

sentiment_tweets_sdf = spark.createDataFrame(sentiment_tweets_df, sentiment_tweets_schema)

sentiment_tweets_sdf.show(5)

"""### Joining Sentiment Tweets and Disasters Data on Apache Spark

We will have sentiment_tweets_sdf left join with disasters_sdf on 'year' and 'month'.

As a result, for the sentiment score and activity of each unique climate change tweet, we can now also observe the number of disasters in each subgroup when that tweet was made.

This will be later important for our models.

Also, we will be removing 'year_created', 'month_created', 'text' because these columns will not be needed for our models.
"""

# Sentiment tweets will be joined with disasters (from earlier) on 'year' and 'month
disasters_sdf.createOrReplaceTempView('disasters')
sentiment_tweets_sdf.createOrReplaceTempView('sentiment_tweets')

query = '''
        SELECT
          st.score AS score,
          st.activity_count AS activity_count,
          d.num_climatological_disasters AS num_climatological_disasters,
          d.num_hydrological_disasters AS num_hydrological_disasters,
          d.num_meteorological_disasters AS num_meteorological_disasters
        FROM sentiment_tweets AS st
          LEFT JOIN disasters AS d
            ON st.year_created = d.year
            AND st.month_created = d.month
        '''

sentiment_tweets_and_disasters_sdf = spark.sql(query)
sentiment_tweets_and_disasters_sdf.show(5)

"""Since there may be no data of disasters for a particular year and month when a climate change tweet was made, we will fill in the nulls with 0.

By filling in the nulls with 0 instead of removing those tweets from our dataframe, we can analyze sentiment score and activity of the tweets for when there are disasters vs when there are no reported disasters.
"""

sentiment_tweets_and_disasters_sdf = sentiment_tweets_and_disasters_sdf.fillna(0)

"""# Model Training & Prediction

Let's start with our climate_tweets_and_disasters data. Since our climate_tweets_and_disasters data is quite large, we will need to use Apache Spark ML for the models we build.

For our sentiment_tweets_and_disasters, although the data is significantly smaller, we also choose to utilize PySpark for consistency reasons -- in other words, in case our model ends up revealing strong predictive capabilities, we can fit the same model to our climate_tweets_and_disasters data.

## Predicting Aggression in Climate Change Tweets (Logistic Regression)

Now, we are going to be using the climate_tweets_and_disasters dataframe to predict whether a tweet contains aggressive language. This can be helpful for content moderators (such as for Twitter or other social media platforms) to quickly identify potential posts that violate terms of use, or other guidelines that they may wish to enforce.

First, we want to set up our features and label dataframes for our logistic regression model.
"""

climate_tweets_and_disasters_sdf = climate_tweets_and_disasters_sdf.withColumnRenamed('aggressiveness_aggressive', 'aggression')

"""### Setting up a VectorAssembler"""

all_columns = climate_tweets_and_disasters_sdf.columns

all_columns

# feature_columns stores the name of the columns to use as features in our model
drop_columns = 'aggression'
all_columns.remove(drop_columns)
feature_columns = all_columns

"""Now, we need to create a VectorAssembler() object to create a feature vector from all of our features, which we have saved as a list called feature_columns.
This feature vector will be then used as input into our Spark ML model, along with a column with our labels -- which in our case, is our "aggression" column.
"""

# Import required library and create VectorAssembler object
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

"""### Pipeline and Train-Test Split

Now that our VectorAssembler object has been initialized with the appropriate parameters, we wish to use it on our Spark dataframe to get the "features" column, which encodes the data from all of our features.

At this point, we need to implement a pipeline so we can use our VectorAssembler to do the above. In our case, we only need a single-stage pipeline -- at this point in our model, anyway (but we will add more stages later on!)

Then, we will create our train and test sets using a 80% training and 20% test split on our data. We will also set our varible random_seed to 13 to ensure consistency.
"""

# # Import required libraries
from pyspark.ml import Pipeline

# Define a pipeline object
pipeline = Pipeline(stages=[assembler])

# Fit and transform the pipeline on the data and store the transformed sdf
model = pipeline.fit(climate_tweets_and_disasters_sdf)
processed_climate_sdf = model.transform(climate_tweets_and_disasters_sdf)

# Assign appropriate value to the random_seed variable
random_seed = 13

# Do 80/20 train-test split with seed = random_seed and store them as "train_sdf" and "test_sdf"
train_sdf, test_sdf = processed_climate_sdf.randomSplit([0.8, 0.2], seed = random_seed)

"""Taking a peek at our train_sdf data, everything looks to be in order! In particular, note how our features column on the far right captures information about the features as a sparse vector since many of the input features are 0, rather than a dense vector, which would actually concatenate the numerical data itself."""

train_sdf.show(5)

"""### Logistic Regression Model

We start by building a standard (unregularized) Logistic Regression model to try to predict whether our tweet will be aggressive, based on its other attributes.

To do this, we will need to import Spark ML's LogisticRegression library for Python, instantiate the Logistic Regression model with the appropriate parameters specific to our data, and then fit the model to our training data.
"""

# import required libraries
from pyspark.ml.classification import LogisticRegression

# Instantiate and fit Logistic Regression model to training data
lr_model = LogisticRegression(featuresCol = 'features', labelCol = 'aggression', maxIter = 5)
clf_model = lr_model.fit(train_sdf)

"""Out of curiosity, let's see which features are the most influential to determining aggression. We want to save the data about our model's coefficients and plot them with our features as labels, which we saved earlier in our "feature_columns" variable."""

coefficients = clf_model.coefficients
print("Coefficients:", coefficients)

plt.figure(figsize = (8,6))
plt.barh(feature_columns, coefficients)
plt.title("Logistic Regression Coefficients for Predicting Aggression")
plt.xlabel("coefficient value")
plt.ylabel("feature")
plt.show()

"""From this bar plot, we see that the features that most positively correlate with aggression are topic_Politics, topic_Donald_Trump_versus_Science, and stance_denier. Considering that discussions around politics can often be polarizing, the first two of these are not a surprise. The model also suggests that tweets that deny climate change are also more likely to be aggressive, which might be expected as well, considering this viewpoint is one that rejects scientific evidence and thus might be written by authors who are more irrational and prone to erratic / problematic behaviors generally (but that's a separate discussion that we'll leave for you, the reader, to ponder).

We also see that tweets with high sentiment, topic_Significance_of_Pollution_Awareness_Events, and topic_Weather_Extremes are among those least likely to be aggressive. The first makes sense, as we can imagine a tweet about one enjoying the unseasonally warm weather, or the like. The second and third might be less frequently aggressive because one might imagine these are the kinds of tweets sent out by news or weather channels, and thus disallow profane language.
"""

# Get training accuracy and store it as `train_accuracy`
train_accuracy = clf_model.summary.accuracy

print("Train Accuracy: " + str(train_accuracy))

"""Our training accuracy is just about 70%. We'll next look at the accuracy on our test data, and since our model was trained on our training data using the '.fit' method, we expect that the accuracy of our model on our test data might be a bit lower."""

# Make predictions on testing set and store it as "predictions"
predictions = clf_model.transform(test_sdf)

"""Now that we have the predictions on out test data, we can analyze how well our model did with a confusion matrix."""

# Import required libraries
from pyspark.mllib.evaluation import MulticlassMetrics

# Select appropriate columns to use MulticlassMetrics
predictionAndLabels = predictions['prediction', 'aggression'].rdd

# Instantiate metrics objects
metrics = MulticlassMetrics(predictionAndLabels)

# Create confusion matrix and store it as a numpy array named "confusion_matrix"
confusion_matrix = metrics.confusionMatrix().toArray()

# Calculate test accuracy using the confusion matrix and store it as "test_accuracy"
TP_0 = confusion_matrix[0][0]
FP_0 = confusion_matrix[0][1]
FN_0 = confusion_matrix[1][0]
TN_0 = confusion_matrix[1][1]

test_accuracy = (TP_0 + TN_0)/(TP_0 + FP_0 + FN_0 + TN_0)
print("Test Accuracy: " + str(test_accuracy))

print(f"True Positive (TP): {int(TP_0)}")
print(f"False Positive (FP): {int(FP_0)}")
print(f"False Negative (FN): {int(FN_0)}")
print(f"True Negative (TN): {int(TN_0)}")

"""Our test accuracy was actually about the same as our training accuracy - 70%. Not bad, but is there opportunity to improve?

To address the possibility of overfitting, let's try regularizing our Logistic Regression model (i.e. L1, L2, and Elastic Net) and see if they return better results. For each regularized model, we will train on the training data and then predict aggression on the test data, and compute accuracy for both the train and test data using confusion matrices.

Overfitting doesn't appear to be a huge issue due the similar accuracies across our test and training data, but it's still worth tweaking our model just in case.

Another concern is underfitting -- i.e, that our model is too simple and our features are not sufficient to capture underlying patterns in aggression. However, we have already used all 24 features included in our dataset, and there's not much we can do to address this.

First, we try L1 (Lasso) regression, which adds a regularization parameter equal to the absolute value of all the coefficients, which can help with selecting only the relevant features to include in our model.

#### L1 (Lasso) Regularization
"""

# L1 (Lasso) Regression Model

# Instantiate LASSO/L1 regularized model as "l1_model"
l1_model = LogisticRegression(featuresCol = 'features', labelCol = 'aggression', maxIter = 5, elasticNetParam= 1.0, regParam = 0.1)

# Fit and Transform using "l1_model"
l1_clf_model = l1_model.fit(train_sdf)
l1_predictions = l1_clf_model.transform(test_sdf)

# Store training accuracy as "l1_train_accuracy"
l1_train_accuracy = l1_clf_model.summary.accuracy
print("Train Accuracy (L1): " + str(l1_train_accuracy))

coefficients = l1_clf_model.coefficients
print("Coefficients:", coefficients)

# Create the Confusion matrix
predictionAndLabels = l1_predictions['prediction', 'aggression'].rdd

metrics = MulticlassMetrics(predictionAndLabels)

confusion_matrix = metrics.confusionMatrix().toArray()

# Calculate the test accuracy as done in the previous section. Store test accuracy as "l1_test_accuracy"
TP_1 = confusion_matrix[0][0]
FP_1 = confusion_matrix[0][1]
FN_1 = confusion_matrix[1][0]
TN_1 = confusion_matrix[1][1]
l1_test_accuracy = (TP_1 + TN_1)/(TP_1 + FP_1 + FN_1 + TN_1)
print("Test Accuracy (L1): " + str(l1_test_accuracy))

print(f"True Positive (TP): {int(TP_1)}")
print(f"False Positive (FP): {int(FP_1)}")
print(f"False Negative (FN): {int(FN_1)}")
print(f"True Negative (TN): {int(TN_1)}")

"""We observe that using a regularized Lasso regression doesn't significantly impact the accuracy of our model on either the training or test data -- which stays just under 70% -- but upon a closer look, it in fact has had an undesirable effect on our model. The penalty term has too greatly simplified our model and shrunk all coefficients to 0, such that our model defaults to predicting 0 (non-aggressive) for all records.

Next, we try using regularized Ridge regression, which minimizes the sum of squared residuals with a regularization term equal to the squared values of the coefficients that penalizes the magnitudes of the coefficients so that they don't become too large. Ridge should not exhibit this behavior as the penalty term will not force coefficients to be exaclty 0.

#### L2 (Ridge) Regularization
"""

# L2 (Ridge) Regression Model

# Instantiate Ridge/L2 regularized model as "l2_model"
l2_model = LogisticRegression(featuresCol = 'features', labelCol = 'aggression', maxIter = 5, elasticNetParam= 0.0, regParam = 0.1)

# Fit and Transform using "l2_model"
l2_clf_model = l2_model.fit(train_sdf)
l2_predictions = l2_clf_model.transform(test_sdf)

# Store training accuracy as "l2_train_accuracy"
l2_train_accuracy = l2_clf_model.summary.accuracy
print("Train Accuracy (L2): " + str(l2_train_accuracy))

coefficients = l2_clf_model.coefficients
print("Coefficients:", coefficients)

# Create the Confusion matrix
predictionAndLabels = l2_predictions['prediction', 'aggression'].rdd

metrics = MulticlassMetrics(predictionAndLabels)

confusion_matrix = metrics.confusionMatrix().toArray()

# Calculate the test accuracy as done in the previous section. Store test accuracy as "l2_test_accuracy"
TP_2 = confusion_matrix[0][0]
FP_2 = confusion_matrix[0][1]
FN_2 = confusion_matrix[1][0]
TN_2 = confusion_matrix[1][1]
l2_test_accuracy = (TP_2 + TN_2)/(TP_2 + FP_2 + FN_2 + TN_2)
print("Test Accuracy (L2): " + str(l2_test_accuracy))

print(f"True Positive (TP): {int(TP_2)}")
print(f"False Positive (FP): {int(FP_2)}")
print(f"False Negative (FN): {int(FN_2)}")
print(f"True Negative (TN): {int(TN_2)}")

"""Indeed, we see that Ridge has not shrunk coefficients to 0; but there's nothing remarkable about the accuracy obtained from Ridge regularization, which still hovers at around 70%. Finally, let's see how a combination of both techniques -- i.e. Elastic Net Regression -- impacts our model.

#### Elastic Net Regularization
"""

# Elastic Net Regression Model

# Instantiate EN regularized model as "en_model"
en_model = LogisticRegression(featuresCol = 'features', labelCol = 'aggression', maxIter = 5, elasticNetParam= 0.5, regParam = 0.1)

# Fit and Transform using "en_model"
en_clf_model = en_model.fit(train_sdf)
en_predictions = en_clf_model.transform(test_sdf)

# Store training accuracy as "en_train_accuracy"
en_train_accuracy = en_clf_model.summary.accuracy
print("Train Accuracy (EN): " + str(en_train_accuracy))

coefficients = en_clf_model.coefficients
print("Coefficients:", coefficients)

# Create the Confusion matrix
predictionAndLabels = en_predictions['prediction', 'aggression'].rdd

metrics = MulticlassMetrics(predictionAndLabels)

confusion_matrix = metrics.confusionMatrix().toArray()

# Calculate the test accuracy as done in the previous section. Store test accuracy as "en_test_accuracy"
TP_en = confusion_matrix[0][0]
FP_en = confusion_matrix[0][1]
FN_en = confusion_matrix[1][0]
TN_en = confusion_matrix[1][1]
en_test_accuracy = (TP_en + TN_en)/(TP_en + FP_en + FN_en + TN_en)
en_test_accuracy
print("Test Accuracy (EN): " + str(en_test_accuracy))

print(f"True Positive (TP): {int(TP_en)}")
print(f"False Positive (FP): {int(FP_en)}")
print(f"False Negative (FN): {int(FN_en)}")
print(f"True Negative (TN): {int(TN_en)}")

"""Running our model with the elasticNetParam set to varying values between 0.3 and 0.7 all reduce our coefficients' values such that the prediction is always 0 for our test data, so like L1 (Lasso), Elastic Net too greatly underfits our model to be useful.

It seems that Lasso, Ridge, and EN Logistic Regressions all failed to improve the predictive accuracy of our models, with accuracy still below 70% for each. What this could mean is that there are other variables at play here that would help determine the aggression of a climate change Tweet, such as the type of account (i.e. personal vs. business) or just the unique character traits of the account user, that simply isn't captured in our dataset.

### Modeling with a Random Forest Classifier

We'll now try using random forests to try to predict the aggression of tweets. Random Forest Regression is an ensemble learning method that aggregates the predictions of multiple decision trees rather than relying on a single decision tree, which generalizes and prevents overfitting while hopefully improving the accuracy of the model.
"""

# Import required libraries
from pyspark.ml.classification import RandomForestClassifier

# Set random_seed to 13
random_seed = 13

# Instantiate the RF Model and call it "rf", then fit it on the training data
rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'aggression', maxDepth = 10,\
                            numTrees = 5, seed = random_seed)
rf_model = rf.fit(train_sdf)

# Get predictions and save to "train_pred" and "test_pred" respectively
train_pred = rf_model.transform(train_sdf)
test_pred = rf_model.transform(test_sdf)

# Evaluate the prediction accuracy for train set and call it "rf_train_accuracy"

predictionAndLabels_train = train_pred['prediction', 'aggression'].rdd
metrics_train = MulticlassMetrics(predictionAndLabels_train)
rf_train_cm = metrics_train.confusionMatrix().toArray()

TP_tr = rf_train_cm[0][0]
FP_tr = rf_train_cm[0][1]
FN_tr = rf_train_cm[1][0]
TN_tr = rf_train_cm[1][1]
rf_train_accuracy = (TP_tr + TN_tr)/(TP_tr + FP_tr + FN_tr + TN_tr)
print("Train Accuracy (RF): " + str(rf_train_accuracy))

# Evaluate the prediction accuracy for test set and call it "rf_test_accuracy"

predictionAndLabels_test = test_pred['prediction', 'aggression'].rdd
metrics_test = MulticlassMetrics(predictionAndLabels_test)
rf_test_cm = metrics_test.confusionMatrix().toArray()

TP_te = rf_test_cm[0][0]
FP_te = rf_test_cm[0][1]
FN_te = rf_test_cm[1][0]
TN_te = rf_test_cm[1][1]
rf_test_accuracy = (TP_te + TN_te)/(TP_te + FP_te + FN_te + TN_te)
print("Test Accuracy (RF): " + str(rf_test_accuracy))

print(f"True Positive (TP): {int(TP_te)}")
print(f"False Positive (FP): {int(FP_te)}")
print(f"False Negative (FN): {int(FN_te)}")
print(f"True Negative (TN): {int(TN_te)}")

"""Using a Random Forest Classifier (with numTrees set to 5 and maxDepth set to 10) bumps our predictive accuracy to around 72%. Other trials include setting numTrees to 10 and maxDepth to 5, but that resulted in sub-70% accuracy; setting both to 10 took an infeasibly long time to execute.

But can we do even better? Let's employ dimensionality reduction using PCA and see if we can continue to improve our model's performance.

### Reducing Dimensions with PCA

Let's try reducing our number of dimensions in our model using Principal Components Analysis to address the potential problem of overfitting. PCA tries to find a new set of axes that the data varies along the most, by calculating the covariance matrix and performing eigenvalue decomposition to generate eigenvectors and their corresponding explained variance values.

We want to fit the PCA class from SparkML onto our training set to find the ideal number of Principal Components, by visualizing the cumulative explained variance by number of components. Then, we will choose the number of principal components for our PCA model accordingly to fit on our training data.
"""

# Import required libraries
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg.distributed import RowMatrix
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import PCA

"""Now, we carry out the steps of performing our Principal Components Analysis, using PySpark's PCA object:

1. Initialize Standard Scaler object to standardize the data. This will ensure that all features have a similar scale before PySpark's PCA object computes the eigenvectors and explained values.
2. Fit and transform our new model, which includes a standard scaler stage, on our train_sdf data. Save this new standardized data as train_sdf_2.
3. Initialize a PCA object and fit it on our standardized train_sdf_2 data to compute its principal eigenvectors, and plot these eigenvectors by their cumulative explained variance to determine an appropriate number to ultimately use in our predictive model.
5. Plot explained variance for varying values of k (from 1 to 24) to determine an appropriate number of components to include in our PCA model, such that the model captures a satisfactory degree of variance without overfitting.
"""

# Initialize StandardScaler object to address scale-variance in PCA
sc = StandardScaler(withMean=True, withStd=False)
sc.setInputCol('features')
sc.setOutputCol('sc_features')

# Fit and our standard scaler stage on our train_sdf data
sc_model = sc.fit(train_sdf)
train_sdf_2 = sc_model.transform(train_sdf)

# fit a PCA with all 24 components to determine the optimal number of components
pca = PCA(k=24, inputCol="sc_features", outputCol="pca_features")
pca_model = pca.fit(train_sdf_2)

# get the cumulative values
cum_values = pca_model.explainedVariance.cumsum()

# plot the graph
plt.figure(figsize=(8,6))
plt.plot(range(1,25), cum_values, marker = 'o', linestyle='--')
plt.ylim(0,1.1)
plt.xlim(1,24)
plt.xticks(np.arange(1, len(cum_values)+1, 1.0))
plt.yticks(np.arange(0, 1.1, 0.1))
plt.title("Cumulative Explained Variance by Number of Components")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.axhline(y=0.95, color='r', linestyle='-')

"""From our analysis of the cumulative explained variance, we can see that 95% of variance is explained by our first 4 components. Thus, we set our optimal k to be 4.

Here, we now transform our standardized training data using a PCA object outfitted with parameter k=4.

Finally, we transform both our training data (saving as test_sdf_3) as well as our test data (saving as test_sdf_3) on this model, and run a Logistic Regression model on our PCA-transformed train and test data to get the PCA model's predictions for aggression.
"""

pca = PCA(k=4, inputCol="sc_features", outputCol="pca_features")
pca_model = pca.fit(train_sdf_2)
train_sdf_3 = pca_model.transform(train_sdf_2)

test_sdf_2 = sc_model.transform(test_sdf)
test_sdf_3 = pca_model.transform(test_sdf_2)

# Instantiate Logistic Regression model and call the model object "lr_model"
model = LogisticRegression(featuresCol = 'pca_features', labelCol = 'aggression', maxIter = 5)
lr_model = model.fit(train_sdf_3)

# Fit Logistic Regression Model and get predictions
predictions = lr_model.transform(test_sdf_3)

# Get training accuracy and store it as `train_accuracy_pca`
train_accuracy_pca = clf_model.summary.accuracy
train_accuracy_pca
print("Train Accuracy (PCA): " + str(train_accuracy_pca))

# Create confusion matrix and store it as "confusion_matrix_pca"
predictionAndLabels_train = predictions['prediction', 'aggression'].rdd
metrics_train = MulticlassMetrics(predictionAndLabels_train)
confusion_matrix_pca = metrics_train.confusionMatrix().toArray()

TP = confusion_matrix_pca[0][0]
FP = confusion_matrix_pca[0][1]
FN = confusion_matrix_pca[1][0]
TN = confusion_matrix_pca[1][1]
test_accuracy_pca = (TP + TN)/(TP + FP + FN + TN)
print("Test Accuracy (PCA): " + str(test_accuracy_pca))

"""In summary, here are the results of the Logisitic Regression:"""

# Display Training Accuracy
print(f"Training Accuracy (PCA): {train_accuracy_pca:.2%}")

# Display Confusion Matrix
print("Confusion Matrix (PCA):")
print(f"        | Predicted 0  | Predicted 1")
print(f"Actual 0| {int(confusion_matrix_pca[0][0]):<12} | {int(confusion_matrix_pca[0][1]):<12}")
print(f"Actual 1| {int(confusion_matrix_pca[1][0]):<12} | {int(confusion_matrix_pca[1][1]):<12}")

# Display TP, FP, FN, TN
print(f"True Positive (TP): {int(TP)}")
print(f"False Positive (FP): {int(FP)}")
print(f"False Negative (FN): {int(FN)}")
print(f"True Negative (TN): {int(TN)}")

# Display Test Accuracy
print(f"Test Accuracy (PCA): {test_accuracy_pca:.2%}")

"""Like Lasso and Elastic Net, reducing our dimensionality to 4 components does not significantly boost our model's predictive accuracy and actually has the adverse effect of predicting all 0's, evidently due to underfitting. This is not surprising, however, since from the outset we hypthosized after running our standard regression model that overfitting was likely not an issue, and in fact that underfitting might be.

## Predicting Activity Generated by Sentiment Tweets

For this part of our project, we wish to understand how one might be able to predict the amount of activity generated by a social media post. For social media platforms that wish to monitor disruptive or harmful content, it would be useful to be able to identify posts that are likely to generate widespread negative impact and remove them before they can go viral and cause harm to other users.

So, we are going to use our sentiment_tweets_and_disasters_sdf data to build a linear regression model to predict the amount of activity a tweet might generate based on the features included in the data.

### Setting up a VectorAssembler

Similar to what we did above for our climate_tweets_and_disasters data, we need to instantiate a VectorAssembler() object to create a feature vector.
This feature vector, along with our "activity_count" column as the label, will be used as inputs to our Spark ML model.
"""

# save column names in 'all_columns'
all_columns = sentiment_tweets_and_disasters_sdf.columns

all_columns

# create feature_columns variable, removing label 'activity_count'
drop_columns = 'activity_count'
all_columns.remove(drop_columns)
feature_columns = all_columns

# instantiate assembler object
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

"""### Pipeline and Train-Test Split

Just as we did for our climate_tweets_and_disasters data, we implement a pipeline to apply the VectorAssember on our sentiment_tweets_and_disaster data to get the "features" column that we need for our Spark ML model.

Once that is done, we create our train and test sets using a 80% training and 20% test split on our data, again setting our varible random_seed to 13 to ensure consistency.
"""

# Define a pipeline object
pipeline = Pipeline(stages=[assembler])

# Fit and transform the pipeline on the data and store the transformed sdf
model = pipeline.fit(sentiment_tweets_and_disasters_sdf)
processed_sentiment_sdf = model.transform(sentiment_tweets_and_disasters_sdf)

# Assign appropriate value to the random_seed variable
random_seed = 13

# Do 80/20 train-test split with seed = random_seed and store them as "train_sdf" and "test_sdf"
train_sdf, test_sdf = processed_sentiment_sdf.randomSplit([0.8, 0.2], seed = random_seed)

train_sdf.show(5)

"""### Linear Regression Model

Rather than use employ a logistic regression classifier, which is more appropriate for binary labels, we aim to use a true linear regression model to try to predict the number of activity actions a tweet might generate.

To do this, we will need to import Spark ML's LinearRegression library for Python, instantiate the Linear Regression model with the appropriate parameters specific to our data, and then fit the model to our training data.
"""

# import required library for Linear Regression
from pyspark.ml.regression import LinearRegression

# Instantiate and fit Linear Regression model to training data
lin_model = LinearRegression(featuresCol = 'features', labelCol = 'activity_count', maxIter = 5, regParam= 0.3)
clf_model = lin_model.fit(train_sdf)

print("Coefficients: " + str(clf_model.coefficients))
print("Intercept: " + str(clf_model.intercept))

trainingSummary = clf_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)
print("explainedVariance: %f" % trainingSummary.explainedVariance)

"""Our R-squared value is very low, indicating that our features (climate-related disasters and sentiment score) are not powerful predictors of a tweet's activity. We expect that running our model on the test data will also yield similar results.

Virality (activity levels) of a social media post is often assumed to involve a lot of randomness, so perhaps these results are not so surprising. If our data included information on the celebrity of the tweets' authors, we might assume that authors with a greater number of followers would generate more activity on their tweets, and thus we might expect our model to have higher predictive power.
"""

# Make predictions on testing set and store it as "predictions"
predictions = clf_model.transform(test_sdf)

"""Below, we visualize the predictions of our data's activity values, and see that our predictions are very far off from the actual values. While our actual data follows something similar to a Zipf (that spikes near the end, due to the virality of a few tweets) where the majority of tweets generate very little activity, our linear regression model is heavily skewed by the handful of tweets that generate hundreds of thousands of actions and thus predicts an average of 5,000 actions per tweet."""

predictions.select("prediction", "activity_count", "features").show(5)

predictions_df = predictions.select("prediction", "activity_count", "features").collect()
predictions_df = pd.DataFrame(predictions_df)

plt.hist(predictions_df[1], log = True)
plt.title('Frequency of Actual Activity Count')
plt.xlabel('Actual Activity')
plt.ylabel('Number of Tweets')
plt.show()

predictions_df.describe()

plt.hist(predictions_df[0])
plt.title('Frequency of Predicted Activity Count: Linear Reg')
plt.xlabel('Predicted Activity')
plt.ylabel('Number of Tweets')
plt.show()

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="activity_count",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(predictions))

"""Now we return to formally evaluating our model. Indeed, our R-squared value is worse on our test data, as we expected would be the case. We now take a look at RMSE, which requires evaluating our model on the test data and recomputing. Somewhat surprisingly, our RMSE value is actually better on the test data."""

test_result = clf_model.evaluate(test_sdf)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)

"""At this point, we assess whether we need to include any kind of regularization to improve our model. However, since our model only has 4 features, high dimensionality should not be an issue. Let's see if features are correlated with a correlation matrix; if not, then there is little chance that including regularization in our model will be very helpful."""

# covariance matrix for countries data
# cols = countries_df.columns;
# countries_df_cleaned = countries_df.dropna(how='any', subset=cols)
# countries_df_cleaned.drop(columns=['Global'], inplace=True)
# covariance_matrix_analysis(countries_df_cleaned, (8,8), "Seat Cover Rate Change BTW Countries")

from pyspark.ml.stat import Correlation

# train_features_vector = train_sdf.select('features')
matrix = Correlation.corr(train_sdf, 'features').collect()[0][0]

corr_matrix = matrix.toArray().tolist()

corr_matrix_df = pd.DataFrame(corr_matrix)

plt.figure(figsize=(6,6))
sns.heatmap(corr_matrix_df, xticklabels=feature_columns, yticklabels=feature_columns, cmap='Greens', annot=True, fmt=".2f")
plt.title('Covariance Between Features in Sentiment Tweets data')
plt.show()

"""Since our correlation matrix does not show strong correlation among any pair of features, with only some limited interaction between num_climatological_disasters with the other natural diasaster counts, we rule out the need to include additional regularization parameters in our linear regression model.

### Random Forest Regression

So far, our models indicate that the amount of engagement / activity generated by social media posts concerning climate change has virtually nothing to do with the post's sentiment score or the number of natural disasters happening concurrently.

Though any relationship between the activity of a tweet and its features is clearly not linear, perhaps there is a non-linear relationship somewhere. So, we employ a random forest to see if there's a non-linear relationship that an ensemble modeling method might uncover.
"""

# import required libraries
from pyspark.ml.regression import RandomForestRegressor

rf = RandomForestRegressor(labelCol = "activity_count", featuresCol = "features", maxDepth = 10, numTrees = 5)

rf_model = rf.fit(train_sdf)

predictions = rf_model.transform(test_sdf)

"""Like in our Linear Regression model, let's visualize the predictions of our model using a histogram. While the results certainly look more reasonable than those from our Linear Regression model, we must also look at R-squared and RMSE stats to see if our features are helpful in predicting a tweet's activity levels."""

predictions_df = predictions.select("prediction", "activity_count", "features").collect()
predictions_df = pd.DataFrame(predictions_df)

plt.hist(predictions_df[0], bins=20)
plt.title('Frequency of Predicted Activity Count: Random Forest Reg')
plt.xlabel('Predicted Activity')
plt.ylabel('Number of Tweets')
plt.xticks(rotation=45, ha='right')
plt.xlim(xmin=0, xmax=max(predictions_df[0])/2)
plt.show()

rf_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="activity_count",metricName="r2")
print("R Squared (R2) on test data = %g" % rf_evaluator.evaluate(predictions))

rf_evaluator = RegressionEvaluator(labelCol="activity_count", predictionCol="prediction", metricName="rmse")
rf_rmse = rf_evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = {}".format(rf_rmse))

"""Again, our model fails to uncover any meaningful relationship between a social media post's activity level and its sentiment score, concurrent natural disasters, or any interaction thereof.

Our interpretation, therefore, is that 1) our model is too simplistic, and we would be well-served by a dataset that includes more information beyond the handful of attributes included within our dataset; and 2) activity generated by social media posts can be somewhat random and unpredictable.

Expanding on 1) -- we might expect that other meaningful predictors of a social media post's activity level would be account-specific information such as the authoring account's number of followers, or time-relevant information such as the time of day or day of the week that the post was published.

As for 2) -- the notion that social media virality is often unpredictable is widely acknowledged, and our model seems to confirm this very belief.

# Model Comparison and Conclusion

Since our project consisted of 2 models -- 1) predicting aggression; and 2) predicting generated activity -- we discuss these separately.

1. Predicting Aggression (using climate_change_and_disasters data)
  * Unregularized logistic regression performed the best, with 69.7% accuracy
  * Lasso, Ridge, and Elastic Net regularization parameters did not improve the accuracy of our model, and in some cases actually decreased accuracy.
  * However, our random forest model did increase accuracy to ~72%. This may be because the specifics of our data set were well-suited for the decision tree structure of prediction.
2. Predicting Activity (using sentiment_and_disasters_data)
  * Our unregularized linear regression showed very low R-squared value and high RMSE, as did our random forest regressor.
  * Ultimately, we interpret this to mean that the attributes available in this data are not good predictors of activity generated.

In conclusion, our analysis revealed that for social media companies that wish to uphold standards of behavior and truthfulness, our models can be used to some extent to predict aggression, but ultimately the virality / engagement generated by a tweet would require further exploration and potentially tracking of more features than what was available to us. As far as tangible auch social media platform monitors could use our model to automate flagging of potentially offensive content

# Challenges & Obstacles Faced

* An obstacle we faced was getting enough tweets related to climate change. Although we had a dataset to work with that contained 3.1+ million climate-change tweets and consisted of many other useful features, our other dataset with the text from the climate-change tweet (97k+ rows) was significantly smaller and contained significantly less features (only usable features: favorite_count, retweet_count, text, created_at). This was a result from the change in Twitter API, which limits us to get only 1500 tweets a month. Additionally, this prevented us from working with the most current tweets as the tweets we worked with were a couple of years old.
* Creation of the state feature: We initally wanted to extract the state location of all our tweets using their latitude and longitude data to do a state analysis and comparison. However, this was not feasible due to the large dataset of tweets we dealt with (3.1+ million). We attempted to achieve this using geopy (with a rate limiter) on apache spark / dask dataframes / pandas multiprocessing but it would run for 30+ min and then break. Therefore, we decided to identify our tweets with 3 regions; West, Central, and East.
* Since we only worked with tweets from the United States, we had to filter our dataset based on longitude/latidue the best we could. We might have excluded and included a few tweets that respectively should and shouldn't have been added in our final dataframes. However, they are such a minority considering the 3.1+ million tweets.
* Because of the large size of the data, climate_tweets_df, we decided to use Apache Spark to join it with the disasters data. This large joined dataframe introduced a blocker when we wanted to plot some visualizations. That is because the function toPandas() failed to convert it into a pandas dataframe.
* Although we attempted to add disasters count as an additional feature for the sentiment tweets, our models did not perform well with the limited features available in the data.

# Potential Next Steps or Future Direction

- We could utilize stream-processing and time-varying data (using Apache Storm) to continuously update our model for predicting whether a tweet has aggressive language. With flow of new daily data to work with, social media platforms can be able to better forecast which types of users to carefully look out for. Social media platforms can better prepare its resources to quickly handle large amounts of incoming tweets (or posts) that may have aggressive language. Thus, this can help prevent misinformation from rapidly spreading.
- We could expand this analysis to multiple social media platforms (Facebook, Reddit...) and compare the users' sentiments and engagements across those various platforms (cross-platform analysis). A county-analysis could also be interesting to understand if climate change engagment on social media is different across the globe. This would introduce the usage of NLP to analyze text in different languages.
"""